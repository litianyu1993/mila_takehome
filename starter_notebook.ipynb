{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f24890b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/paligemma/lib/python3.12/site-packages (2.2.2)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/envs/paligemma/lib/python3.12/site-packages (3.10.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/envs/paligemma/lib/python3.12/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/envs/paligemma/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/envs/paligemma/lib/python3.12/site-packages (from matplotlib) (4.59.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/envs/paligemma/lib/python3.12/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in /opt/anaconda3/envs/paligemma/lib/python3.12/site-packages (from matplotlib) (2.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/paligemma/lib/python3.12/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/envs/paligemma/lib/python3.12/site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/envs/paligemma/lib/python3.12/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/anaconda3/envs/paligemma/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/paligemma/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: torch in /opt/anaconda3/envs/paligemma/lib/python3.12/site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/paligemma/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/envs/paligemma/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/paligemma/lib/python3.12/site-packages (from torch) (72.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/envs/paligemma/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/paligemma/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/paligemma/lib/python3.12/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/envs/paligemma/lib/python3.12/site-packages (from torch) (2025.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/paligemma/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/paligemma/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: torchvision in /opt/anaconda3/envs/paligemma/lib/python3.12/site-packages (0.20.1)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/paligemma/lib/python3.12/site-packages (from torchvision) (2.2.2)\n",
      "Requirement already satisfied: torch in /opt/anaconda3/envs/paligemma/lib/python3.12/site-packages (from torchvision) (2.5.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/envs/paligemma/lib/python3.12/site-packages (from torchvision) (11.1.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/paligemma/lib/python3.12/site-packages (from torch->torchvision) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/envs/paligemma/lib/python3.12/site-packages (from torch->torchvision) (4.12.2)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/paligemma/lib/python3.12/site-packages (from torch->torchvision) (72.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/envs/paligemma/lib/python3.12/site-packages (from torch->torchvision) (1.13.1)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/paligemma/lib/python3.12/site-packages (from torch->torchvision) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/paligemma/lib/python3.12/site-packages (from torch->torchvision) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/envs/paligemma/lib/python3.12/site-packages (from torch->torchvision) (2025.2.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/paligemma/lib/python3.12/site-packages (from sympy==1.13.1->torch->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/paligemma/lib/python3.12/site-packages (from jinja2->torch->torchvision) (3.0.2)\n",
      "Requirement already satisfied: transformers in /opt/anaconda3/envs/paligemma/lib/python3.12/site-packages (4.49.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/paligemma/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /opt/anaconda3/envs/paligemma/lib/python3.12/site-packages (from transformers) (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/paligemma/lib/python3.12/site-packages (from transformers) (2.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/paligemma/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/paligemma/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/paligemma/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/paligemma/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/envs/paligemma/lib/python3.12/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/envs/paligemma/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/envs/paligemma/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/paligemma/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/paligemma/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/paligemma/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/paligemma/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/paligemma/lib/python3.12/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/paligemma/lib/python3.12/site-packages (from requests->transformers) (2025.7.14)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/paligemma/lib/python3.12/site-packages (4.67.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install torch\n",
    "!pip install torchvision\n",
    "!pip install transformers\n",
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf5e385e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/paligemma/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose\n",
    "from torchvision.datasets import CIFAR100\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9deda8cf",
   "metadata": {},
   "source": [
    "# Image Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9079d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageTransformerEncoder(nn.Module):\n",
    "    def __init__(self, image_size=(32, 64), patch_size=16, embed_dim=512, mlp_dim=1024, num_layers=1, num_heads=1):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = (image_size[0] // patch_size) * (image_size[1] // patch_size)\n",
    "        self.patch_dim = 3 * patch_size * patch_size  # RGB channels\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Patch embedding\n",
    "        self.patch_embedding = nn.Linear(self.patch_dim, embed_dim)\n",
    "        \n",
    "        # Positional encoding\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_patches, embed_dim))\n",
    "        \n",
    "        # Class token\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "        \n",
    "        blocks = []\n",
    "        for _ in range(num_layers):\n",
    "            blocks.append(nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=mlp_dim,\n",
    "            dropout=0.1,\n",
    "            activation='relu',\n",
    "            batch_first=True\n",
    "        ))\n",
    "        self.layers = nn.ModuleList(blocks)\n",
    "\n",
    "        self.final_ln = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        \n",
    "    def patchify(self, x):\n",
    "        \"\"\"Convert image to patches\"\"\"\n",
    "        B, C, H, W = x.shape\n",
    "        assert H % self.patch_size == 0\n",
    "        assert W % self.patch_size == 0\n",
    "        x = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\n",
    "        x = x.contiguous().view(B, C, -1, self.patch_size, self.patch_size)\n",
    "        x = x.permute(0, 2, 1, 3, 4).contiguous()\n",
    "        x = x.view(B, self.num_patches, -1)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        \n",
    "        # Convert to patches\n",
    "        patches = self.patchify(x)  # (B, num_patches, patch_dim)\n",
    "        \n",
    "        # Patch embedding\n",
    "        patch_embeddings = self.patch_embedding(patches)  # (B, num_patches, embed_dim)\n",
    "        # Add positional encoding\n",
    "        patch_embeddings = patch_embeddings + self.pos_embedding\n",
    "\n",
    "        # Add class token\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
    "        x = torch.cat([cls_tokens, patch_embeddings], dim=1)\n",
    "                \n",
    "        for block in self.layers:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.final_ln(x)\n",
    "        # Return class token as feature vector\n",
    "        cls = x[:, 0]  # (B, embed_dim)\n",
    "\n",
    "        return cls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873c39fd",
   "metadata": {},
   "source": [
    "# Contrastive Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f5a1de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contrastive loss function\n",
    "def contrastive_loss(image_features, text_features, temperature=0.01):\n",
    "    # Normalize features\n",
    "    image_features = F.normalize(image_features, dim=-1)\n",
    "    text_features = F.normalize(text_features, dim=-1)\n",
    "    \n",
    "    # Compute cosine similarity matrix\n",
    "    logits = torch.matmul(image_features, text_features.t()) / temperature\n",
    "    \n",
    "    # Labels for contrastive learning (diagonal should be positive pairs)\n",
    "    batch_size = image_features.shape[0]\n",
    "    labels = torch.arange(batch_size, device=image_features.device)\n",
    "    \n",
    "    # Cross-entropy loss for both directions\n",
    "    loss_i2t = F.cross_entropy(logits, labels)\n",
    "    loss_t2i = F.cross_entropy(logits.t(), labels)\n",
    "    \n",
    "    return (loss_i2t + loss_t2i) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f4c84f",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e46a64c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CIFAR100PairedWithCaption(Dataset):\n",
    "    \"\"\"\n",
    "    Returns: (stacked_image, caption, (label_left, label_right))\n",
    "    - stacked_image: torch.Tensor, shape (C, H, 2*W)\n",
    "    - caption: str, e.g. \"the photo on the left is apple and the photo on the right is bus\"\n",
    "    - labels: tuple of ints\n",
    "    \"\"\"\n",
    "    def __init__(self, root=\"./data\", train=True, transform=None, download=True):\n",
    "        self.cifar_dataset = CIFAR100(\n",
    "            root=root,\n",
    "            train=train,\n",
    "            transform=None,\n",
    "            download=download\n",
    "        )\n",
    "        self.transform = transform\n",
    "        self.class_names = self.cifar_dataset.classes\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cifar_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Pick first image\n",
    "        img1, label1 = self.cifar_dataset[idx]\n",
    "        # Pick a random second image (could be the same as first)\n",
    "        idx2 = torch.randint(high=len(self.cifar_dataset), size=(1,)).item()\n",
    "        img2, label2 = self.cifar_dataset[idx2]\n",
    "\n",
    "        # Apply transforms if provided\n",
    "        if self.transform:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "        else:\n",
    "            # Convert PIL to tensor if no transform\n",
    "            img1 = ToTensor()(img1)\n",
    "            img2 = ToTensor()(img2)\n",
    "\n",
    "        # Stack horizontally: shape (C, H, 2*W)\n",
    "        stacked_img = torch.cat([img1, img2], dim=2)\n",
    "\n",
    "        # Caption\n",
    "        class1 = self.class_names[label1]\n",
    "        class2 = self.class_names[label2]\n",
    "        caption = f\"the photo on the left is {class1} and the photo on the right is {class2}\"\n",
    "\n",
    "        return stacked_img, caption, (label1, label2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c361be",
   "metadata": {},
   "source": [
    "# Set seed and device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f39504c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        # For completely deterministic results, may impact performance\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def worker_init_fn(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "\n",
    "def get_device() -> torch.device:\n",
    "    if torch.backends.mps.is_available():\n",
    "        return torch.device(\"mps\")\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "\n",
    "    return torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5f74d4",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd614feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the custom datasets\n",
    "def create_caption_dataloaders(dataset_class, batch_size=128, root=\"./data\", seed=42):\n",
    "    \"\"\"Create dataloaders that return image-caption pairs\"\"\"\n",
    "    \n",
    "    # Data transforms\n",
    "    transform = Compose([\n",
    "        ToTensor(),\n",
    "        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Create custom datasets\n",
    "    train_dataset = dataset_class(\n",
    "        root=root,\n",
    "        train=True,\n",
    "        transform=transform,\n",
    "        download=True\n",
    "    )\n",
    "\n",
    "    # seed dataloaders    \n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(seed)\n",
    "\n",
    "    # Create dataloaders\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=0, \n",
    "        pin_memory=True,\n",
    "        worker_init_fn=worker_init_fn,\n",
    "        generator=g\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        pin_memory=True,\n",
    "        worker_init_fn=worker_init_fn,\n",
    "        generator=g\n",
    "    )\n",
    "    \n",
    "    return train_loader, test_loader, train_dataset.class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e585fe9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Train loader: 6250 batches\n",
      "Val loader: 6250 batches\n",
      "Classes: 100\n"
     ]
    }
   ],
   "source": [
    "set_seed(seed=42)\n",
    "my_device = get_device()\n",
    "\n",
    "# get train and val dataloaders\n",
    "bt_size = 8\n",
    "train_loader, val_loader, class_names = create_caption_dataloaders(dataset_class=CIFAR100PairedWithCaption,\n",
    "                                                                    batch_size=bt_size)\n",
    "\n",
    "print(f\"Train loader: {len(train_loader)} batches\")\n",
    "print(f\"Val loader: {len(val_loader)} batches\")\n",
    "print(f\"Classes: {len(class_names)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9187da29",
   "metadata": {},
   "source": [
    "# Training and Validation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a108020",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(text_to_encode, text_encoder, tokenizer, device):\n",
    "    inputs = tokenizer(text_to_encode, \n",
    "                       padding=True, \n",
    "                       truncation=True, \n",
    "                       return_tensors=\"pt\")\n",
    "    inputs = inputs.to(device)    \n",
    "    outputs = text_encoder(**inputs)\n",
    "    text_features = outputs.pooler_output\n",
    "    \n",
    "    return text_features\n",
    "\n",
    "\n",
    "def train_step(images, texts, optimizer, image_encoder,\n",
    "               text_encoder, tokenizer, device):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Get image features\n",
    "    images = images.to(device)\n",
    "    image_features = image_encoder(images)\n",
    "    \n",
    "    # Get text features\n",
    "    with torch.no_grad():\n",
    "        text_features = encode_text(texts,  text_encoder, tokenizer, device)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = contrastive_loss(image_features, text_features)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()\n",
    "\n",
    "def train_epoch(image_encoder, text_encoder, tokenizer, \n",
    "                train_loader, epoch, optimizer, device):\n",
    "    image_encoder.train()\n",
    "    text_encoder.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=f'Epoch {epoch}')\n",
    "    \n",
    "    for batch_idx, (images, captions, label) in enumerate(progress_bar):\n",
    "        \n",
    "        # Forward pass and compute loss\n",
    "        loss = train_step(images, captions, optimizer, image_encoder,\n",
    "                          text_encoder, tokenizer, device)\n",
    "        \n",
    "        total_loss += loss\n",
    "        num_batches += 1\n",
    "        \n",
    "        # Update progress bar\n",
    "        progress_bar.set_postfix({'Loss': f'{loss:.4f}', 'Avg Loss': f'{total_loss/num_batches:.4f}'})\n",
    "    \n",
    "    return total_loss / num_batches\n",
    "\n",
    "\n",
    "def validate_epoch(image_encoder, text_encoder, tokenizer, \n",
    "                              val_loader, device):\n",
    "    image_encoder.eval()\n",
    "    text_encoder.eval()\n",
    "    total_loss = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, captions, labels) in enumerate(val_loader):            \n",
    "            # Get features\n",
    "            images = images.to(device)\n",
    "            image_features = image_encoder(images)\n",
    "            text_features = encode_text(captions, text_encoder, tokenizer, device)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = contrastive_loss(image_features, text_features)\n",
    "\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            num_batches += 1\n",
    "    \n",
    "    return total_loss / num_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ea0090",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753637e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO NOT modify the evaluation function\n",
    "def evaluate_topk(image_encoder, text_encoder, tokenizer, \n",
    "                val_loader, class_names, device):\n",
    "    \"\"\"\n",
    "    Zero-shot evaluation: For each image, predict the closest caption from the prompts\n",
    "    Returns top-1, top-10, top-100 recall.\n",
    "    \"\"\"\n",
    "    image_encoder.eval()\n",
    "    text_encoder.eval()\n",
    "    # Prepare all combinations\n",
    "    prompts = []\n",
    "    for class_name_left in class_names: # 100x100\n",
    "        for class_name_right in class_names:\n",
    "            prompts.append(f\"the photo on the left is {class_name_left} and the photo on the right is {class_name_right}\")\n",
    "    assert len(prompts) == len(set(prompts)), \"Prompts must be unique!\"\n",
    "    # map prompt to index\n",
    "    prompt_to_id = {}\n",
    "    for i, prompt in enumerate(prompts):\n",
    "        prompt_to_id[prompt] = i\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # batch prompts to reduce peak memory\n",
    "        batch_prompts = 256\n",
    "        txt_features = []\n",
    "        for index in range(0, len(prompts), batch_prompts):\n",
    "            current_prompts = prompts[index: index + batch_prompts]\n",
    "            text_features = encode_text(current_prompts, text_encoder, tokenizer, device)  \n",
    "            # normalize \n",
    "            text_features = F.normalize(text_features, dim=-1)\n",
    "            # aggregate \n",
    "            txt_features.append(text_features)\n",
    "        # stack\n",
    "        text_features = torch.concatenate(txt_features, dim=0)\n",
    "        assert text_features.size(0) == len(prompts)\n",
    "\n",
    "        top1, top10, top100, total = 0, 0, 0, 0\n",
    "\n",
    "        for images, captions, labels in tqdm(val_loader, desc=\"Topk eval\"):\n",
    "            images = images.to(device)\n",
    "            # Encode images\n",
    "            image_features = image_encoder(images)\n",
    "            image_features = F.normalize(image_features, dim=-1)\n",
    "\n",
    "            # Compute similarity (batch_size, num_classes)\n",
    "            logits = image_features @ text_features.T\n",
    "\n",
    "            # Top-1 and Top-5 predictions\n",
    "            top1_pred = logits.argmax(dim=-1)\n",
    "            top10_pred = logits.topk(10, dim=-1).indices\n",
    "            top100_pred = logits.topk(100, dim=-1).indices\n",
    "\n",
    "            # get_labels\n",
    "            idx_relative_to_prompt = [] \n",
    "            for cap in captions:\n",
    "                idx_relative_to_prompt.append(prompt_to_id[cap])\n",
    "            idx_relative_to_prompt = torch.tensor(idx_relative_to_prompt, device=device)\n",
    "\n",
    "            top1 += (top1_pred == idx_relative_to_prompt).sum().item()\n",
    "            top10 += sum([idx_relative_to_prompt[i] in top10_pred[i] for i in range(idx_relative_to_prompt.size(0))])\n",
    "            top100 += sum([idx_relative_to_prompt[i] in top100_pred[i] for i in range(idx_relative_to_prompt.size(0))])\n",
    "\n",
    "            total += idx_relative_to_prompt.size(0)\n",
    "\n",
    "    top1_acc = 100 * top1 / total\n",
    "    top10_acc = 100 * top10 / total\n",
    "    top100_acc = 100 * top100 / total\n",
    "    print(f\"Zero-shot Top-1 Acc: {top1_acc:.2f}%, Top-10 Acc: {top10_acc:.2f}% Top-100 Acc: {top100_acc:.2f}%\")\n",
    "    return top1_acc, top10_acc, top100_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db20c72",
   "metadata": {},
   "source": [
    "# Load Image and Text Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10ceb890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageTransformerEncoder(\n",
       "  (patch_embedding): Linear(in_features=768, out_features=512, bias=True)\n",
       "  (layers): ModuleList(\n",
       "    (0): TransformerEncoderLayer(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
       "      )\n",
       "      (linear1): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (linear2): Linear(in_features=1024, out_features=512, bias=True)\n",
       "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout1): Dropout(p=0.1, inplace=False)\n",
       "      (dropout2): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (final_ln): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load CLIP text encoder and tokenizer\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "text_encoder.to(my_device)\n",
    "\n",
    "# models\n",
    "# Create an instance of the encoder\n",
    "image_encoder = ImageTransformerEncoder()\n",
    "image_encoder.to(my_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a404566",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "98a6da23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Training on 6250 batches per epoch\n",
      "Validating on 6250 batches per epoch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 6250/6250 [02:12<00:00, 47.08it/s, Loss=1.5194, Avg Loss=1.8511]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "Train Loss: 1.8511, Val Loss: 1.6413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Topk eval: 100%|██████████| 6250/6250 [01:03<00:00, 98.67it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-shot Top-1 Acc: 0.13%, Top-10 Acc: 1.16% Top-100 Acc: 7.81%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 6250/6250 [02:18<00:00, 44.99it/s, Loss=1.4111, Avg Loss=1.6771]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/4\n",
      "Train Loss: 1.6771, Val Loss: 1.5752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Topk eval: 100%|██████████| 6250/6250 [01:03<00:00, 97.78it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-shot Top-1 Acc: 0.16%, Top-10 Acc: 1.31% Top-100 Acc: 8.83%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 6250/6250 [02:17<00:00, 45.45it/s, Loss=1.4530, Avg Loss=1.6215]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/4\n",
      "Train Loss: 1.6215, Val Loss: 1.4755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Topk eval: 100%|██████████| 6250/6250 [01:04<00:00, 97.58it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-shot Top-1 Acc: 0.19%, Top-10 Acc: 1.49% Top-100 Acc: 10.46%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 6250/6250 [02:18<00:00, 45.20it/s, Loss=2.1695, Avg Loss=1.5835]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/4\n",
      "Train Loss: 1.5835, Val Loss: 1.4419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Topk eval: 100%|██████████| 6250/6250 [01:04<00:00, 97.54it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-shot Top-1 Acc: 0.21%, Top-10 Acc: 1.68% Top-100 Acc: 10.98%\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "parameters = list(image_encoder.parameters())\n",
    "op = optim.Adam(parameters, lr=1e-4)\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 4\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(f\"Training on {len(train_loader)} batches per epoch\")\n",
    "print(f\"Validating on {len(val_loader)} batches per epoch\")\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Train\n",
    "    train_loss = train_epoch(image_encoder, text_encoder, tokenizer, \n",
    "                             train_loader, epoch + 1, op, my_device)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss = validate_epoch(image_encoder, text_encoder, tokenizer, \n",
    "                              val_loader, my_device)\n",
    "    \n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # # Evaluate retrieval\n",
    "    top1_acc, top10_acc, top100_acc = evaluate_topk(image_encoder, text_encoder, tokenizer, \n",
    "                                                    val_loader, class_names, my_device)\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "    \n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c605e1",
   "metadata": {},
   "source": [
    "# Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "130c8d82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('output/clip_text_encoder/tokenizer_config.json',\n",
       " 'output/clip_text_encoder/special_tokens_map.json',\n",
       " 'output/clip_text_encoder/vocab.json',\n",
       " 'output/clip_text_encoder/merges.txt',\n",
       " 'output/clip_text_encoder/added_tokens.json')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save image encoder (architecture + weights)\n",
    "os.makedirs(\"output\", exist_ok=True)  # Creates the directory if it doesn't exist\n",
    "torch.save(image_encoder, \"output/image_encoder.pth\")\n",
    "\n",
    "# Save text encoder and tokenizer (weights + config)\n",
    "text_encoder.save_pretrained(\"output/clip_text_encoder\")\n",
    "tokenizer.save_pretrained(\"output/clip_text_encoder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5c818b",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c891dd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CLIPTextModel(\n",
       "  (text_model): CLIPTextTransformer(\n",
       "    (embeddings): CLIPTextEmbeddings(\n",
       "      (token_embedding): Embedding(49408, 512)\n",
       "      (position_embedding): Embedding(77, 512)\n",
       "    )\n",
       "    (encoder): CLIPEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x CLIPEncoderLayer(\n",
       "          (self_attn): CLIPSdpaAttention(\n",
       "            (k_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (v_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (q_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "            (out_proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): CLIPMLP(\n",
       "            (activation_fn): QuickGELUActivation()\n",
       "            (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          )\n",
       "          (layer_norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load image encoder (architecture + weights)\n",
    "image_encoder = torch.load(\"output/image_encoder.pth\", weights_only=False)\n",
    "image_encoder.to(my_device)\n",
    "\n",
    "# Load text encoder and tokenizer (weights + config)\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"output/clip_text_encoder\")\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"output/clip_text_encoder\")\n",
    "text_encoder.to(my_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e47b59",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7ebf2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Topk eval: 100%|██████████| 6250/6250 [01:04<00:00, 97.64it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zero-shot Top-1 Acc: 0.24%, Top-10 Acc: 1.69% Top-100 Acc: 10.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# # Evaluate retrieval\n",
    "top1_acc, top10_acc, top100_acc = evaluate_topk(image_encoder, text_encoder, tokenizer, \n",
    "                                                val_loader, class_names, my_device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paligemma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
